### 内容提要

* 本章主要讲解了web机器人一些原理和介绍，以及怎样控制机器人的访问和业界的一些关于跟踪机器人的规范！

#### 概念

* Web机器人是能够在无需人类干预的情况下自动进行一系列Web事务处理的软件程序。人们根据这些机器人探查web站点的方式，形象的给它们取了一个饱含特色的名字，比如“爬虫”、“蜘蛛”、“蠕虫”以及“机器人”等！

#### 爬虫及爬行方式

* “爬虫”主要采取的爬行方式是获取第一个web页面，然后递归地对各种信息性web站点进行遍历，从而获取相关页面。搜索引擎的爬虫是一些复杂的爬虫，因为他们不仅会爬行web页面，而且会把相关数据拉取回来建立数据库，方便用户搜索！

1. 爬虫会从根集开始爬行

2. 爬虫会解析页面所有的url，并把它们转换绝对形式

3. 要避免环路的出现，因为这些环路会暂停或减缓机器人的爬行过程

* 环路对爬虫有害的三个原因：

1. 爬虫会陷入循环之中，从而兜圈子，浪费带宽，无法获取新页面！

2. 爬虫无限的请求服务器，从而阻塞了真正的用户去请求服务器，这是可以作为法律诉讼理由的！

3. 爬虫服务器会被重复的数据充斥

* 网络中两个url表面上看起来不一样，但是指向的是同一资源，那么这两个url就互相称为“别名”，由于别名问题的存在，所以爬虫会爬行重复的数据，所以爬虫有必要把url的进行规范化！从而解决相关数据重复问题。相关规范方法如：没有端口默认为80，把字符转义为等价字符，删除#标签等。


#### 机器人的HTTP

* 相关首部

>> User-Agent ：机器人名字

>> From ：提供机器人管理者的E-mail地址

>> Accept : 告知服务器可以发送那些媒体类型

>> Referer ：提供包含了当前请求的URL的文档的URL

* 虚拟主机需要爬虫带Host首部，要不然会返回错误主机的数据

* 让爬虫使用条件请求是有意义的，因为有的数据内容没有改变，所以重复抓取是浪费空间的，只有在内容实际改变的时候才重新发起请求，即条件请求。

#### 行为不当的机器人

* 失控的机器人，比正常用户的请求速度快很多，当这类爬虫设计出现错误的时候，很容易短时间之内增加服务器的负载，阻止真正用户的访问，原因诸如：编程逻辑错误、陷入环路之中

* 失效的url，url可能已经失效了，但是爬虫依然取请求它 ，这样会让服务器的日志文档里面增加了很多请求出错的记录。

* 很长的错误url,同样请求这样一个url，会让服务器日志文档增加一个很杂论的出错记录

* 爱打听的机器人，访问了一些管理者不允许访问的内容，涉及侵犯隐私

* 动态网关访问


#### 拒绝机器人访问

* 通过一个叫robots.txt的文件来约束机器人的访问。它的思想就是指定那些部分机器人可以访问，那些部分机器人不能访问
。如果机器人遵循这个自愿约束标准，那么在请求所有资源之前，它需要获取robots.txt并解析它。

* 请求robots.txt时针对服务器返回的状态码，爬虫所作的动作：

>> 如果返回2xx代码，机器人就必须对内容进行解析，并使用排斥规则从那个站点上获取内容

>> 如果返回404，机器人认为服务器没有激活排斥规则，所以它不受限制


>> 如果返回401或403(访问限制)，表示机器人是完全受限的

>> 如果返回503（服务器临时故障），那么机器人暂时停止访问，知道正常之后继续请求robots.txt

>> 如果返回重定向代码，那么机器人也应该重定向到相关页面

* robots.txt文件的格式：包括三种内容注释行、空行、规则行。如：

``` javascript

	# this robots.txt file allows Slurp & Webcrawler to crawl
	# the public parts of our site,but no other robots...

	User-Agent: slurp
	User-Agent: webcraler
	Disallow: /private

	User-Agent: *
	Disallow:

```

